---
title: "Normalizing Flows"
author: "Abhisek Banerjee"
from: markdown+raw_html
execute:
  echo: false

filters:
  - quarto

# IMPORTANT:    
html-math-method: katex

format:
  revealjs:
    theme: [default, custom.scss]
    css: styles.css

    # Slide geometry
    width: 1280
    height: 720
    margin: 0.05
    center: false

    # Navigation & behavior
    transition: slide
    background-transition: fade
    slide-number: true
    progress: true
    controls: true
    hash: true
    preview-links: true
    incremental: false
    chalkboard: true
    code-line-numbers: true

    # Math rendering
    html-math-method: katex

    # Tiger-stripe background ONLY for title slide
    title-slide-attributes:
      data-background-image: "assets/tigerstripes.png"
      data-background-size: "cover"
      data-background-position: "center"

    # Fonts & head includes
    include-in-header:
      text: |
        <script src="https://cdn.jsdelivr.net/npm/d3@7"></script>
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <link rel="preconnect" href="https://fonts.googleapis.com">
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;600;800;900&display=swap" rel="stylesheet">

    # Footer + Home button
    include-after-body:
      text: |
        <div class="mizzou-footer-bar">
          <div class="mizzou-left">
            <img src="assets/mizzou-logo.svg" alt="Mizzou logo">
          </div>
          <div class="mizzou-right">
            Department of Statistics / Class 9100
          </div>
        </div>

        <a class="home-btn" href="#outline" title="Go to Outline" aria-label="Go to Outline">
          <svg viewBox="0 0 24 24" aria-hidden="true">
            <path d="M3 10.5L12 3l9 7.5V21a1 1 0 0 1-1 1h-5v-6H9v6H4a1 1 0 0 1-1-1z"/>
          </svg>
        </a>
        - flow-demo.js

resources:
  - styles.css
  - assets
---


## Outline {#outline}


<div class="toc-card">

1. [Motivation](#motivation)
2. [Normalizing Flows](#NormFlows)  
3. [Inference with Normalizing Flows](#InfNormFlow)  
4. [Algorithm of Normalizing Flows](#AlgoNormFlows) 
5. [Bayes Flow](#BFlowStart)  
6. [Algorithm of Bayes Flow](#AlgoBayesFlow)
7. [Recent Developments in Normalizing Flows](#DevelopNormFlow)
8. [Summary of Normalizing Flow Families](#SummNormFlow)
9. [Future Directions of Normalizing Flows](#FutureDir)

</div>


## Introduction 

- Many real-world distributions are:
  - **non-Gaussian**, **skewed or heavy-tailed**, **multi-modal and correlated**
- Classical models (Gaussian, mixtures) can be **too restrictive**
- We want models that are:
  - **Flexible**, **Exact likelihood**, **Efficient to train and sample**


### Core Idea

- Start with a **simple base distribution** $z_0 \sim \mathcal{N}(0, I)$
- Apply a sequence of **invertible, differentiable transformations** $z_0 \xrightarrow{f_1} z_1 \xrightarrow{f_2} \cdots \xrightarrow{f_K} x$
- The resulting distribution of $x$ can be **highly complex**



### Why ‚ÄúNormalizing‚Äù Flow?

- Each transformation **pushes probability mass**
- The density evolves smoothly through the transformations
- Using the **change-of-variables formula**, we get:
  - Exact likelihoods & Tractable training via gradient descent



---

## Motivation  {#motivation}

### Variational inference: the core problem

We consider a latent variable model with observations $x$ and latent variables $z$ and parameter $\theta$.

The posterior
$$
p_\theta(z \mid x)
$$
is typically **intractable**.


### Variational idea  

Introduce a tractable approximation
$$
q_\phi(z \mid x) \approx p_\theta(z \mid x),
$$
and optimize the evidence lower bound (ELBO):
$$
\log p_\theta(x)
\;\ge\;
\mathbb{E}_{q_\phi(z \mid x)}\!\left[\log p_\theta(x \mid z)\right]
-
\mathrm{D_{KL}}\!\left(q_\phi(z \mid x)\,\|\,p(z)\right).
$$



---

## Reparameterization trick

Assume a Gaussian variational posterior:
$$
q_\phi(z \mid x) = \mathcal{N}\!\big(z | \mu(x), \sigma^2(x)\big).
$$

<br>

### Reparameterize

Instead of sampling $z \sim q_\phi$, write:
$$
z = \mu(x) + \sigma(x)\,\varepsilon,
\qquad
\varepsilon \sim \mathcal{N}(0, I).
$$

<br>

### Result

Gradients move **inside** the expectation:
$$
\nabla_\phi
\mathbb{E}_{q_\phi(z \mid x)}[f(z)]
=
\mathbb{E}\!\left[
\nabla_\phi f\big(\mu(x)+\sigma(x)\varepsilon\big)
\right].
$$
---

## The problem with standard amortized VI

In practice, $q_\phi(z \mid x)$ is chosen to be **simple**:

$$
q_\phi(z \mid x)
=
\mathcal{N}\!\big(z | \mu_\phi(x), \mathrm{diag}(\sigma_\phi^2(x))\big).
$$



### Consequences

- Mean-field independence assumptions
- Cannot represent:
  - multimodality
  - strong correlations
  - complex posterior geometry



### Empirical issues (well documented)

- Underestimation of posterior variance
- Biased parameter estimates
- Poor uncertainty quantification


---

## What do we want from a variational posterior?

An ideal variational family should be:

- Flexible enough to approximate the true posterior
- Compatible with reparameterization
- Scalable with amortized inference
- Computationally tractable

<br>

### Key question

Can we start with a simple distribution
and **systematically increase its complexity**?

<br>

```{ojs}


flow_strip_anim = {
  const W = 920, H = 170, pad = 10;
  const panels = 4, panelW = W / panels;

  // sample base points
  const n = 320;
  const pts0 = d3.range(n).map(() => {
    const u1 = Math.random(), u2 = Math.random();
    const r = Math.sqrt(-2*Math.log(u1));
    const a = 2*Math.PI*u2;
    return [r*Math.cos(a), r*Math.sin(a)];
  });

  // STRONGER, visually distinct warps
  const f1 = ([x,y]) => [x + 1.2*Math.tanh(1.2*y), y];                   // big shear
  const f2 = ([x,y]) => {
    const th = 0.55*Math.tanh(0.9*x);                                    // local rotation
    const c = Math.cos(th), s = Math.sin(th);
    return [c*x - s*y, s*x + c*y];
  };
  const f3 = ([x,y]) => {
    const r = Math.sqrt(x*x + y*y) + 1e-6;
    const k = 0.9*Math.tanh(1.0*(r-1.1));                                // twist/radial
    return [x + k*(-y), y + k*(x)];
  };

  const steps = [
    {name: "z‚ÇÄ", pts: pts0},
    {name: "z‚ÇÅ", pts: pts0.map(f1)},
    {name: "z‚ÇÇ", pts: pts0.map(f1).map(f2)},
    {name: "z‚ÇÉ", pts: pts0.map(f1).map(f2).map(f3)}
  ];

  const xS = d3.scaleLinear().domain([-4,4]).range([pad, panelW-pad]);
  const yS = d3.scaleLinear().domain([-4,4]).range([H-pad, pad]);

  const svg = d3.create("svg")
    .attr("viewBox", `0 0 ${W} ${H}`)
    .style("width","100%")
    .style("max-width","920px")
    .style("height","170px")
    .style("border","1px solid rgba(255,255,255,0.18)")
    .style("border-radius","10px");

  // panel frames + labels
  steps.forEach((s, i) => {
    const g = svg.append("g").attr("transform", `translate(${i*panelW},0)`);

    g.append("rect")
      .attr("x", 2).attr("y", 2)
      .attr("width", panelW-4).attr("height", H-4)
      .attr("rx", 10)
      .attr("fill", "transparent")
      .attr("stroke", "rgba(255,255,255,0.12)");

    g.append("text")
      .attr("x", 10).attr("y", 18)
      .attr("font-size", 14)
      .attr("font-weight", 650)
      .text(s.name);

    if (i < panels-1) {
      svg.append("text")
        .attr("x", (i+1)*panelW - 10)
        .attr("y", H/2 + 6)
        .attr("font-size", 22)
        .attr("opacity", 0.55)
        .text("‚Üí");
    }
  });

  // draw dots in each panel (same indices, so we can interpolate)
  const dotGroups = steps.map((s, i) => {
    return svg.append("g")
      .attr("transform", `translate(${i*panelW},0)`)
      .selectAll("circle")
      .data(pts0.map((p, idx) => ({idx, p0: p})))
      .join("circle")
        .attr("r", 1.35)
        .attr("opacity", 0.75);
  });

  // helper: interpolate between step a and b
  function lerp(a,b,t){ return a + (b-a)*t; }

  function render(tms){
    // stage cycles: (0‚Üí1) for z0->z1, then z1->z2, then z2->z3
    const cycle = (tms/1200) % 3;             // 0..3
    const k = Math.floor(cycle);              // 0,1,2
    const t = cycle - k;                      // 0..1

    for (let i=0; i<steps.length; i++){
      // Each panel shows its "target" step, but animates the transition for the current stage
      let from = i, to = i;

      if (k === 0 && i === 1) { from = 0; to = 1; }   // animate z1
      if (k === 1 && i === 2) { from = 1; to = 2; }   // animate z2
      if (k === 2 && i === 3) { from = 2; to = 3; }   // animate z3

      const A = steps[from].pts;
      const B = steps[to].pts;

      dotGroups[i]
        .attr("cx", d => {
          const a = A[d.idx], b = B[d.idx];
          return xS(lerp(a[0], b[0], t));
        })
        .attr("cy", d => {
          const a = A[d.idx], b = B[d.idx];
          return yS(lerp(a[1], b[1], t));
        });
    }

    requestAnimationFrame(render);
  }

  requestAnimationFrame(render);
  return svg.node();
}
``` 


## Normalizing flows  {#NormFlows}


<br>

<div class="mizzou-gold-box">

A **normalizing flow** constructs a complex density by transforming
a simple one through invertible mappings.

By repeatedly applying the rule for change of variables, the initial density ‚Äòflows‚Äô through the sequence of invertible mappings.

</div>

<br>

### Construction

Start with
$$
z_0 \sim q_0(z_0 \mid x),
$$
and apply a sequence of invertible transformations:
$$
z_k = f_k(z_{k-1}), \qquad k=1,\dots,K.
$$

The final variable $z_K$ has density $q_K(z_K \mid x)$.

---

## Change of variables

By the change-of-variables formula:
$$
\log q_K(z_K \mid x)
=
\log q_0(z_0 \mid x)
-
\sum_{k=1}^K
\log\left|
\det
\frac{\partial f_k}{\partial z_{k-1}}
\right|.
$$



### Interpretation

- Each transformation **warps** the density
- Stacking transformations yields rich posteriors
- Complexity controlled by flow length $K$

### Why normalizing flows help

- Start from a simple, reparameterizable base distribution
- Add flexibility through invertible transformations
- Preserve tractable likelihoods and gradients



---

## Expectations under a flow (LOTUS)
Let
$$
f:\mathbb{R}^d \to \mathbb{R}^d
$$
Then,
$$
z_K = f_K \circ f_{K-1} \circ \cdots \circ f_1(z_0),
$$

A key property:

For any function $h(z)$,
$$
\mathbb{E}_{q_K}[h(z)]
=
\mathbb{E}_{q_0}\!\left[
h\big(f_K \circ \cdots \circ f_1(z_0)\big)
\right].
$$

<br>

### Consequence

- We can compute expectations **without explicitly evaluating** $q_K$
- Jacobian terms only matter when evaluating $\log q_K$

---

## Geometric intuition

Effect of invertible flows can be thought of as a sequence of expansions or contractions on the initial density

- **Expansions**  
  ‚Üí The map $z'=f(z)$ pulls the pointz $z$ away from a region in $\mathbb{R}^d$, reducing density in that region.

- **Contractions**  
  ‚Üí concentrate mass, increase density locally


By composing expansions and contractions,
a simple base density can be warped into a highly complex one.

```{ojs}
d3 = require("d3@7")

flow_viz = {
  // ‚Üì‚Üì‚Üì reduced size ‚Üì‚Üì‚Üì
  const W = 520, H = 260, pad = 18;

  const xs = d3.scaleLinear().domain([-2,2]).range([pad, W-pad]);
  const ys = d3.scaleLinear().domain([-1.4,1.4]).range([H-pad, pad]);

  // ‚Üì‚Üì‚Üì slightly sparser grid ‚Üì‚Üì‚Üì
  const pts = [];
  for (let x=-2; x<=2.001; x+=0.22)
    for (let y=-1.4; y<=1.4001; y+=0.22)
      pts.push([x,y]);

  const svg = d3.create("svg")
    .attr("viewBox", `0 0 ${W} ${H}`)
    .style("width","100%")
    .style("max-width","520px")
    .style("height","260px")
    .style("border","1px solid rgba(255,255,255,0.18)")
    .style("border-radius","10px");

  const label = svg.append("text")
    .attr("x", 10).attr("y", 18)
    .attr("font-size", 14)
    .attr("font-weight", 600);

  const dots = svg.selectAll("circle")
    .data(pts)
    .join("circle")
      .attr("r", 1.6)
      .attr("opacity", 0.75);

  function frame(tms){
    const t = (Math.sin(tms/900) + 1) / 2;
    const strength = (2*t - 1) * 0.55;

    label.text(strength >= 0
      ? "Expansion"
      : "Contraction"
    );

    const warped = pts.map(([x0,y0]) => {
      const r2 = x0*x0 + y0*y0 + 0.15;
      const s = strength / r2;
      return [x0 + s*x0, y0 + s*y0];
    });

    dots
      .data(warped)
      .attr("cx", d => xs(d[0]))
      .attr("cy", d => ys(d[1]));

    requestAnimationFrame(frame);
  }

  requestAnimationFrame(frame);
  return svg.node();
}

```


## Infinitestimal Flows

We define the variational posterior as
$$
q_\phi(z \mid x) := q_K(z_K),
$$

where $f_k$ are the transformations.


### Key idea

Complexity is controlled by **flow length** $K$,


What happens as the flow length $K \to \infty$?


Instead of discrete transformations,
the density evolves continuously in time:
$$
\frac{\partial}{\partial t} q_t(z) = \mathcal{T}_t[q_t(z)],
$$

where $\mathcal{T}_t$ defines continuous-time dynamics.


---

## Infinitestimal Flows: A Demo

```{ojs}
infinitesimal_flow_slider = {
  const W = 920, H = 420, pad = 16;

  // -----------------------------
  // Controls
  // -----------------------------
  const nInput = Inputs.range([10, 10000], {
    label: "Sample Size",
    step: 10,
    value: 1000
  });

  const tInput = Inputs.range([0, 1], {
    label: "Flow time t",
    step: 0.001,
    value: 0.0
  });

  const speedInput = Inputs.range([0.0, 2.5], {
    label: "Auto-play speed",
    step: 0.05,
    value: 0.0
  });

  const ui = html`
<style>
  .nf-controls {
    font-family: system-ui, -apple-system, Segoe UI, Roboto, sans-serif;
  }

  .nf-controls label {
    font-size: 11px !important;
    font-weight: 500;
    opacity: 0.75;
  }

  .nf-controls input[type="range"] {
    height: 18px;
  }

  /* Default numeric boxes */
  .nf-controls input[type="number"] {
    font-size: 11px !important;
    height: 22px;
    width: 72px;
    padding: 2px 4px;
    box-sizing: border-box;
    overflow: visible;
    text-overflow: clip;
    white-space: nowrap;
    text-align: right;
  }

  /* üëá Make SAMPLE SIZE box wider */
  .nf-controls .sample-size input[type="number"] {
    width: 120px;
  }

  /* Smaller spinner arrows */
  .nf-controls input[type="number"]::-webkit-inner-spin-button,
  .nf-controls input[type="number"]::-webkit-outer-spin-button {
    transform: scale(0.7);
    margin: 0;
  }
</style>

<div class="nf-controls"
     style="display:grid;
            grid-template-columns: 1fr 1fr 1fr;
            gap:12px;
            max-width:${W}px;
            align-items:end;">
  <div class="sample-size">${nInput}</div>
  <div>${tInput}</div>
  <div>${speedInput}</div>
</div>
`;

  // -----------------------------
  // SVG
  // -----------------------------
  const svg = d3.create("svg")
    .attr("viewBox", `0 0 ${W} ${H}`)
    .style("width", "100%")
    .style("max-width", `${W}px`)
    .style("height", `${H}px`)
    .style("border", "1px solid rgba(255,255,255,0.18)")
    .style("border-radius", "12px");

  svg.append("rect")
    .attr("x", 0).attr("y", 0)
    .attr("width", W).attr("height", H)
    .attr("fill", "rgba(255,255,255,0.92)");

  svg.append("text")
    .attr("x", 14).attr("y", 28)
    .attr("font-size", 16)
    .attr("font-weight", 700)
    .attr("fill", "rgba(0,0,0,0.82)")
    .text("Infinitesimal Flow: points evolve continuously with time t");

 
  const plot = svg.append("g");

  // -----------------------------
  // Scales
  // -----------------------------
  const xS = d3.scaleLinear().domain([-4, 4]).range([pad, W - pad]);
  const yS = d3.scaleLinear().domain([-3.5, 3.5]).range([H - pad, 70]);

  // grid
  const grid = plot.append("g").attr("opacity", 0.18);
  for (let gx = -4; gx <= 4; gx++)
    grid.append("line")
      .attr("x1", xS(gx)).attr("x2", xS(gx))
      .attr("y1", yS(-3.5)).attr("y2", yS(3.5))
      .attr("stroke", "black");

  for (let gy = -3; gy <= 3; gy++)
    grid.append("line")
      .attr("x1", xS(-4)).attr("x2", xS(4))
      .attr("y1", yS(gy)).attr("y2", yS(gy))
      .attr("stroke", "black");

  // -----------------------------
  // Helpers
  // -----------------------------
  function randn() {
    let u = 0, v = 0;
    while (!u) u = Math.random();
    while (!v) v = Math.random();
    return Math.sqrt(-2 * Math.log(u)) * Math.cos(2 * Math.PI * v);
  }

  function vField(t, x, y) {
    const cx = 0.9 * Math.sin(2 * Math.PI * t);
    const cy = 0.6 * Math.cos(2 * Math.PI * t);
    const dx = x - cx, dy = y - cy;
    const swirl = 1.05;
    const squeeze = 0.65 * Math.sin(2 * Math.PI * t);
    const bend = 0.35 * Math.tanh(x * y);
    return [
      -swirl * dy + squeeze * x + bend,
       swirl * dx - squeeze * y - 0.25 * Math.tanh(x)
    ];
  }

  function rk2Step(t, x, y, dt) {
    const [k1x, k1y] = vField(t, x, y);
    const xm = x + 0.5 * dt * k1x;
    const ym = y + 0.5 * dt * k1y;
    const [k2x, k2y] = vField(t + 0.5 * dt, xm, ym);
    return [x + dt * k2x, y + dt * k2y];
  }

  function evolve(points0, tEnd) {
    const dt = 0.01;
    const steps = Math.max(1, Math.ceil(tEnd / dt));
    const h = tEnd / steps;
    let pts = points0.map(p => p.slice());
    let t = 0;
    for (let s = 0; s < steps; s++) {
      for (let i = 0; i < pts.length; i++) {
        const [x, y] = pts[i];
        pts[i] = rk2Step(t, x, y, h);
      }
      t += h;
    }
    return pts;
  }

  // -----------------------------
  // State + draw
  // -----------------------------
  let pts0 = [];
  let circles = plot.append("g").selectAll("circle");

  function resample(n) {
    pts0 = d3.range(n).map(() => [randn(), 0.85 * randn()]);
  }

  function draw(n, t) {
    if (pts0.length !== n) resample(n);
    const ptsT = t === 0 ? pts0 : evolve(pts0, t);

    circles = circles
      .data(ptsT, (_, i) => i)
      .join("circle")
      .attr("r", 1.35)
      .attr("opacity", 0.75)
      .attr("cx", d => xS(d[0]))
      .attr("cy", d => yS(d[1]));
  }

  draw(nInput.value, tInput.value);

  nInput.addEventListener("input", () => {
    resample(nInput.value);
    draw(nInput.value, tInput.value);
  });

  tInput.addEventListener("input", () => {
    draw(nInput.value, tInput.value);
  });

  let last = null;
  function tick(ts) {
    if (last === null) last = ts;
    const dtms = ts - last;
    last = ts;
    if (speedInput.value > 0) {
      let t = tInput.value + speedInput.value * dtms / 1000;
      if (t > 1) t -= 1;
      tInput.value = t;
      draw(nInput.value, t);
    }
    requestAnimationFrame(tick);
  }
  requestAnimationFrame(tick);

  return html`<div style="display:grid; gap:10px;">
    ${ui}
    ${svg.node()}
  </div>`;
}
```






---


::: columns
::: column
### Langevin flow

One important infinitesimal flow is the Langevin SDE:
$$
dz(t) = F(z(t),t)\,dt + G(z(t),t)\,d\xi(t),
$$

where $d\xi(t)$ is a Wiener process, $F$ is a drift vector, $D=GG'$ is a diffusion matrix. 


<br>
In machine learning, commonly:
$$
F(z,t) = -\nabla_z \mathcal{L}(z),
\qquad
G(z,t) = \sqrt{2}\,I.
$$
where $\mathcal{L}(z)$ is a unnormalised log-density of the model.
:::



::: column
### Hamiltonian flows

Hamiltonian Monte Carlo can also be viewed as a normalizing flow
on an augmented space $(z,\omega)$.

<br>

Its dynamics results from Hamiltonian $$\mathcal{H}(z,w)=-\mathcal{L}(z)-\frac{1}{2}\omega'M\omega$$

<br>

Also widely used in machine learning.

:::
:::

---

## Langevin vs Hamiltonian flows

```{ojs}
langevin_vs_hamiltonian_story4 = {
  const W = 920, H = 380, pad = 18, panelW = W / 2;

  // -------------------------------------------------
  // Controls (stacked)
  // -------------------------------------------------
  const speed = Inputs.range([0, 3.0], { label: "Auto-play speed", step: 0.05, value: 1.4 });
  const noise = Inputs.range([0, 1.0], { label: "Langevin noise (shake)", step: 0.02, value: 0.45 });
  const friction = Inputs.range([0, 0.25], { label: "Hamiltonian friction (0‚âàHMC ideal)", step: 0.01, value: 0.03 });

  // NEW: cycle length for "uphill ‚Üí downhill" repeats
  const cycleSeconds = Inputs.range([2, 12], { label: "Reset cycle (seconds): re-drop particles uphill", step: 1, value: 6 });

  const ui = html`
<style>
  .flow-controls { font-family: system-ui, -apple-system, Segoe UI, Roboto, sans-serif; }
  .flow-controls .row { display:block; margin: 0 0 8px 0; }
  .flow-controls label { font-size: 11px; opacity: 0.75; }
  .flow-controls input[type="number"] { font-size: 11px; width: 92px; height: 22px; }
  .flow-controls input[type="range"] { height: 18px; width: 100%; }
</style>
<div class="flow-controls" style="max-width:${W}px;">
  <div class="row">${speed}</div>
  <div class="row">${noise}</div>
  <div class="row">${friction}</div>
  <div class="row">${cycleSeconds}</div>
</div>`;

  // -------------------------------------------------
  // SVG + panels
  // -------------------------------------------------
  const svg = d3.create("svg")
    .attr("viewBox", `0 0 ${W} ${H}`)
    .style("width", "100%")
    .style("max-width", `${W}px`)
    .style("height", `${H}px`)
    .style("border", "1px solid rgba(255,255,255,0.18)")
    .style("border-radius", "12px")
    .style("background", "rgba(255,255,255,0.96)");

  const gL = svg.append("g").attr("transform", `translate(0,0)`);
  const gR = svg.append("g").attr("transform", `translate(${panelW},0)`);

  svg.append("line")
    .attr("x1", panelW).attr("x2", panelW)
    .attr("y1", 0).attr("y2", H)
    .attr("stroke", "rgba(0,0,0,0.10)");

  gL.append("text")
    .attr("x", 14).attr("y", 24)
    .attr("font-size", 16).attr("font-weight", 750)
    .text("Langevin: downhill drift + random shaking");

  gR.append("text")
    .attr("x", 14).attr("y", 24)
    .attr("font-size", 16).attr("font-weight", 750)
    .text("Hamiltonian: momentum (smooth, long moves)");

  gL.append("text")
    .attr("x", 14).attr("y", 46)
    .attr("font-size", 12).attr("opacity", 0.70)
    .text("drift = ‚àí‚àáU(z) (downhill), noise = shake  ‚Üí repeated resets show ‚Äúroll down‚Äù");

  gR.append("text")
    .attr("x", 14).attr("y", 46)
    .attr("font-size", 12).attr("opacity", 0.70)
    .text("leapfrog on U(z) with momentum p (inertia) ‚Üí long, smooth descents");

  // -------------------------------------------------
  // Scales
  // -------------------------------------------------
  const xDom = [-3.6, 3.6], yDom = [-2.9, 2.9];
  const xS = d3.scaleLinear().domain(xDom).range([pad, panelW - pad]);
  const yS = d3.scaleLinear().domain(yDom).range([H - pad, 70]);

  // -------------------------------------------------
  // Helpers
  // -------------------------------------------------
  function randn() {
    let u = 0, v = 0;
    while (!u) u = Math.random();
    while (!v) v = Math.random();
    return Math.sqrt(-2 * Math.log(u)) * Math.cos(2 * Math.PI * v);
  }
  const clamp = (a, lo, hi) => Math.max(lo, Math.min(hi, a));

  // Strong bowl so ‚Äúdownhill‚Äù is visually clear; high density ~ exp(-U)
  function U(x, y) {
    const bowl = 0.60 * (x * x + 1.35 * y * y);
    const ripples = 0.18 * Math.sin(1.7 * x) * Math.cos(1.25 * y);
    return bowl + ripples;
  }
  function gradU(x, y) {
    const e = 1e-3;
    const ux = (U(x + e, y) - U(x - e, y)) / (2 * e);
    const uy = (U(x, y + e) - U(x, y - e)) / (2 * e);
    return [ux, uy];
  }

  // -------------------------------------------------
  // Background: density shading + contours
  // -------------------------------------------------
  function drawDensity(group) {
    const nx = 140, ny = 100;

    const Uvals = new Array(nx * ny);
    let k = 0, umin = +Infinity, umax = -Infinity;

    for (let j = 0; j < ny; j++) {
      const yy = d3.interpolate(yDom[0], yDom[1])(j / (ny - 1));
      for (let i = 0; i < nx; i++) {
        const xx = d3.interpolate(xDom[0], xDom[1])(i / (nx - 1));
        const u = U(xx, yy);
        Uvals[k++] = u;
        if (u < umin) umin = u;
        if (u > umax) umax = u;
      }
    }

    const plotX0 = pad, plotY0 = 70;
    const plotW = panelW - 2 * pad;
    const plotH = (H - pad) - 70;

    const cellW = plotW / (nx - 1);
    const cellH = plotH / (ny - 1);

    const shade = group.append("g").attr("opacity", 0.22);
    k = 0;
    for (let j = 0; j < ny - 1; j++) {
      for (let i = 0; i < nx - 1; i++) {
        const u = Uvals[k++];
        const t = (u - umin) / (umax - umin + 1e-12);
        const alpha = 1 - t; // darker = higher density
        shade.append("rect")
          .attr("x", plotX0 + i * cellW)
          .attr("y", plotY0 + j * cellH)
          .attr("width", cellW + 0.2)
          .attr("height", cellH + 0.2)
          .attr("fill", `rgba(0,0,0,${0.55 * alpha})`);
      }
      k++;
    }

    const thresholds = d3.range(0.8, 10.0, 0.60);
    const cont = d3.contours().size([nx, ny]).thresholds(thresholds)(Uvals);

    const path = d3.geoPath(
      d3.geoIdentity()
        .scale(plotW / (nx - 1))
        .translate([plotX0, plotY0])
    );

    group.append("g")
      .attr("opacity", 0.22)
      .selectAll("path")
      .data(cont)
      .join("path")
        .attr("d", path)
        .attr("fill", "none")
        .attr("stroke", "rgba(0,0,0,0.45)")
        .attr("stroke-width", 1);
  }

  drawDensity(gL);
  drawDensity(gR);

  function frame(group) {
    group.append("rect")
      .attr("x", pad).attr("y", 70)
      .attr("width", panelW - 2 * pad)
      .attr("height", (H - pad) - 70)
      .attr("fill", "none")
      .attr("stroke", "rgba(0,0,0,0.12)")
      .attr("rx", 8);
  }
  frame(gL); frame(gR);

  // -------------------------------------------------
  // Particle init: start "uphill" (far from center)
  // -------------------------------------------------
  const N = 20;
  const trailLen = 60;

  function sampleUphill() {
    const ang = 2 * Math.PI * Math.random();
    const r = 2.6 + 0.6 * Math.random(); // far away
    const x = r * Math.cos(ang) + 0.20 * randn();
    const y = 0.95 * r * Math.sin(ang) + 0.20 * randn();
    return [clamp(x, xDom[0] + 0.1, xDom[1] - 0.1), clamp(y, yDom[0] + 0.1, yDom[1] - 0.1)];
  }

  let zL, zR, pR, trailL, trailR;

  function resetAll() {
    zL = d3.range(N).map(sampleUphill);
    zR = d3.range(N).map(sampleUphill);

    // Hamiltonian momentum: strong so motion is obvious immediately
    pR = d3.range(N).map(() => [1.8 * randn(), 1.8 * randn()]);

    trailL = zL.map(p => [p.slice()]);
    trailR = zR.map(p => [p.slice()]);
  }
  resetAll();

  // -------------------------------------------------
  // Layers
  // -------------------------------------------------
  const trailsGroupL = gL.append("g");
  const trailsGroupR = gR.append("g");

  const dotsL = gL.append("g").selectAll("circle").data(d3.range(N)).join("circle")
    .attr("r", 3).attr("opacity", 0.92);

  const dotsR = gR.append("g").selectAll("circle").data(d3.range(N)).join("circle")
    .attr("r", 3).attr("opacity", 0.92);

  const arrowsR = gR.append("g").selectAll("line").data(d3.range(N)).join("line")
    .attr("stroke", "rgba(0,0,0,0.40)")
    .attr("stroke-width", 1.7)
    .attr("stroke-linecap", "round");

  const driftArrowsL = gL.append("g").selectAll("line").data(d3.range(N)).join("line")
    .attr("stroke", "rgba(0,0,0,0.35)")
    .attr("stroke-width", 1.2)
    .attr("stroke-linecap", "round")
    .attr("opacity", 0.75);

  const line = d3.line()
    .x(d => xS(d[0]))
    .y(d => yS(d[1]));

  function render() {
    trailsGroupL.selectAll("path")
      .data(trailL)
      .join("path")
        .attr("d", d => line(d))
        .attr("fill", "none")
        .attr("stroke", "rgba(0,0,0,0.22)")
        .attr("stroke-width", 1.2);

    trailsGroupR.selectAll("path")
      .data(trailR)
      .join("path")
        .attr("d", d => line(d))
        .attr("fill", "none")
        .attr("stroke", "rgba(0,0,0,0.22)")
        .attr("stroke-width", 1.2);

    dotsL
      .attr("cx", i => xS(zL[i][0]))
      .attr("cy", i => yS(zL[i][1]));

    dotsR
      .attr("cx", i => xS(zR[i][0]))
      .attr("cy", i => yS(zR[i][1]));

    // Langevin drift arrows: -‚àáU (downhill)
    const driftScale = 0.26;
    driftArrowsL
      .attr("x1", i => xS(zL[i][0]))
      .attr("y1", i => yS(zL[i][1]))
      .attr("x2", i => {
        const [ux, uy] = gradU(zL[i][0], zL[i][1]);
        return xS(zL[i][0] - driftScale * ux);
      })
      .attr("y2", i => {
        const [ux, uy] = gradU(zL[i][0], zL[i][1]);
        return yS(zL[i][1] - driftScale * uy);
      });

    // Hamiltonian momentum arrows
    const arrowScale = 0.22;
    arrowsR
      .attr("x1", i => xS(zR[i][0]))
      .attr("y1", i => yS(zR[i][1]))
      .attr("x2", i => xS(zR[i][0] + arrowScale * pR[i][0]))
      .attr("y2", i => yS(zR[i][1] + arrowScale * pR[i][1]));
  }

  // -------------------------------------------------
  // Dynamics
  // -------------------------------------------------
  function step(dt) {
    const sig = +noise.value;
    const gam = +friction.value;

    // Langevin: roll downhill + shake
    for (let i = 0; i < N; i++) {
      let [x, y] = zL[i];
      const [ux, uy] = gradU(x, y);

      const driftStrength = 1.25; // strong inward drift so ‚Äúuphill‚Üídownhill‚Äù is obvious
      x += -driftStrength * dt * ux + Math.sqrt(2 * dt) * sig * randn();
      y += -driftStrength * dt * uy + Math.sqrt(2 * dt) * sig * randn();

      x = clamp(x, xDom[0] + 0.05, xDom[1] - 0.05);
      y = clamp(y, yDom[0] + 0.05, yDom[1] - 0.05);

      zL[i] = [x, y];
      trailL[i].push([x, y]);
      if (trailL[i].length > trailLen) trailL[i].shift();
    }

    // Hamiltonian: leapfrog (smooth) + tiny friction, reflect boundaries
    for (let i = 0; i < N; i++) {
      let [x, y] = zR[i];
      let [px, py] = pR[i];

      // p half-step
      let [ux, uy] = gradU(x, y);
      px += -0.5 * dt * ux;
      py += -0.5 * dt * uy;

      // z full-step
      x += dt * px;
      y += dt * py;

      // reflect boundaries
      if (x < xDom[0]) { x = xDom[0] + (xDom[0] - x); px *= -1; }
      if (x > xDom[1]) { x = xDom[1] - (x - xDom[1]); px *= -1; }
      if (y < yDom[0]) { y = yDom[0] + (yDom[0] - y); py *= -1; }
      if (y > yDom[1]) { y = yDom[1] - (y - yDom[1]); py *= -1; }

      // p half-step
      [ux, uy] = gradU(x, y);
      px += -0.5 * dt * ux;
      py += -0.5 * dt * uy;

      // friction
      px *= (1 - gam);
      py *= (1 - gam);

      // keep motion visible
      const pnorm = Math.sqrt(px * px + py * py);
      if (pnorm < 0.22) {
        px += 1.2 * randn();
        py += 1.2 * randn();
      }

      zR[i] = [x, y];
      pR[i] = [px, py];

      trailR[i].push([x, y]);
      if (trailR[i].length > trailLen) trailR[i].shift();
    }
  }

  // -------------------------------------------------
  // Auto-play loop with periodic "uphill reset"
  // This creates: uphill ‚Üí downhill, then reset uphill ‚Üí downhill, ...
  // -------------------------------------------------
  let last = null;
  let phase = 0; // seconds into current cycle

  function tick(ts) {
    if (last == null) last = ts;
    const dtms = ts - last;
    last = ts;

    const s = +speed.value;
    const dtSec = (dtms / 1000) * s;
    const dt = 0.012 * (dtms / 16.67) * s; // integration step

    // update phase and reset periodically
    phase += dtSec;
    const T = +cycleSeconds.value;

    if (phase >= T) {
      phase = 0;
      resetAll();      // <-- this gives the repeated ‚Äúuphill ‚Üí downhill‚Äù behavior
    }

    // integrate with a couple substeps for smoothness
    const sub = 2;
    for (let k = 0; k < sub; k++) step(dt / sub);

    render();
    requestAnimationFrame(tick);
  }

  render();
  requestAnimationFrame(tick);

  return html`<div style="display:grid; gap:10px;">
    ${ui}
    ${svg.node()}
  </div>`;
}
```



---

## Inference with normalizing flows {#InfNormFlow}

### Invertible linear-time transformations
<br>

To construct expressive variational posteriors, we need transformations that are:

- **Invertible** (to define a valid density)
- **Differentiable** (for gradients)
- **Computationally efficient** (Jacobian determinant is cheap)

<br>



<br>

<div class="mizzou-gold-box">
The key idea is to design flows whose Jacobian determinants can be computed in **linear time** $O(d)$.
</div>

---

## Invertible Linear-time Transformations


Consider transformations of the form:
$$
f(z) = z + u\, h(w^\top z + b),
$$

where $u, w \in \mathbb{R}^d$, $b \in \mathbb{R}$ $\&$ $h(\cdot)$ is a smooth scalar nonlinearity

### Jacobian determinant (planar flow)

Define
$$
\psi(z) = h'(w^\top z + b)\, w.
$$

Then the Jacobian determinant simplifies to:
$$
\left| \det \frac{\partial f}{\partial z} \right|
=
\left| 1 + u^\top \psi(z) \right|.
$$


### Consequences

Determinant computed in **$O(d)$ time**

Density is tractable:
$\log q_K(z_K)
=
\log q_0(z_0)
-
\sum_{k=1}^K
\log | 1 + u_k^\top \psi_k(z_k) |$

---

## Flow-based free energy bound

We parameterize the approximate posterior using a flow of length \(K\):
$$
q_\phi(z \mid x) := q_K(z_K),
\qquad
z_K = f_K \circ \cdots \circ f_1(z_0),
\quad
z_0 \sim q_0(z_0).
$$

<br>

### Free energy (ELBO)

The variational free energy can be written as an expectation
over the **base distribution** $q_0$:
$$
\mathcal{F}(x)
=
\mathbb{E}_{q_0(z_0)}\!\left[
\log q_0(z_0)
-
\log p(x, z_K)
-
\sum_{k=1}^K
\log \left| 1 + u_k^\top \psi_k(z_k) \right|
\right].
$$

<br>

### Interpretation

- Optimization is performed **in the base space** $z_0$
- Normalizing flows and this free energy bound can be used with any variational optimization scheme, including generalized variational EM.

---

## Algorithm summary (normalizing flows) {#AlgoNormFlows}

Normalizing flows extend **amortized variational inference**
by augmenting the variational posterior with a sequence of
invertible transformations.


```{ojs}

algo_diagram = {
  const W = 980, H = 260;

  const svg = d3.create("svg")
    .attr("viewBox", `0 0 ${W} ${H}`)
    .style("width","100%")
    .style("max-width","980px")
    .style("height","260px");

  // ---- layout ----
  const nodes = [
    {id:"mb",  x:60,  y:60,  w:170, h:54, title:"Mini-batch",   sub:"x ‚Üê {get mini-batch}"},
    {id:"q0",  x:270, y:60,  w:200, h:54, title:"Base sample",  sub:"z‚ÇÄ ~ q‚ÇÄ(z‚ÇÄ | x)"},
    {id:"flow",x:520, y:60,  w:240, h:54, title:"Flow map",     sub:"z_K = f_K‚àò‚Ä¶‚àòf‚ÇÅ(z‚ÇÄ)"},
    {id:"F",   x:810, y:60,  w:150, h:54, title:"Free energy",  sub:"ùìï(x) ‚âà ùìï(x,z_K)"},

    {id:"gth", x:620, y:170, w:210, h:54, title:"Update Œ∏",     sub:"ŒîŒ∏ ‚àù ‚àí‚àá_Œ∏ ùìï(x)"},
    {id:"gph", x:360, y:170, w:210, h:54, title:"Update œï",     sub:"Œîœï ‚àù ‚àí‚àá_œï ùìï(x)"}
  ];

  const edges = [
    ["mb","q0"], ["q0","flow"], ["flow","F"],
    ["F","gth"], ["F","gph"]
  ];

  // ---- defs: arrow ----
  svg.append("defs").append("marker")
    .attr("id","arrow")
    .attr("viewBox","0 0 10 10")
    .attr("refX", 9).attr("refY", 5)
    .attr("markerWidth", 8).attr("markerHeight", 8)
    .attr("orient", "auto-start-reverse")
    .append("path")
    .attr("d","M 0 0 L 10 5 L 0 10 z")
    .attr("fill","rgba(0,0,0,0.45)");

  // helper
  const byId = new Map(nodes.map(d => [d.id, d]));
  const centerRight = d => [d.x + d.w, d.y + d.h/2];
  const centerLeft  = d => [d.x,       d.y + d.h/2];
  const centerTop   = d => [d.x + d.w/2, d.y];
  const centerBot   = d => [d.x + d.w/2, d.y + d.h];

  // ---- edges ----
  svg.append("g")
    .selectAll("path")
    .data(edges)
    .join("path")
      .attr("fill","none")
      .attr("stroke","rgba(0,0,0,0.35)")
      .attr("stroke-width", 2)
      .attr("marker-end","url(#arrow)")
      .attr("d", ([a,b]) => {
        const A = byId.get(a), B = byId.get(b);

        // horizontal main chain
        if (["mb","q0","flow"].includes(a) && ["q0","flow","F"].includes(b)) {
          const [x1,y1] = centerRight(A);
          const [x2,y2] = centerLeft(B);
          const mx = (x1+x2)/2;
          return `M${x1},${y1} C${mx},${y1} ${mx},${y2} ${x2},${y2}`;
        }

        // down from F to updates
        if (a === "F") {
          const [x1,y1] = centerBot(A);
          const [x2,y2] = centerTop(B);
          const mx = (x1+x2)/2;
          return `M${x1},${y1} C${x1},${y1+30} ${x2},${y2-30} ${x2},${y2}`;
        }
        return "";
      });

  // ---- nodes ----
  const g = svg.append("g").selectAll("g.node")
    .data(nodes)
    .join("g")
      .attr("class","node")
      .attr("transform", d => `translate(${d.x},${d.y})`);

  g.append("rect")
    .attr("width", d => d.w)
    .attr("height", d => d.h)
    .attr("rx", 14)
    .attr("fill", "rgba(255,255,255,0.85)")
    .attr("stroke", "rgba(0,0,0,0.18)")
    .attr("stroke-width", 1.5);

  g.append("text")
    .attr("x", 14).attr("y", 22)
    .attr("font-size", 14)
    .attr("font-weight", 800)
    .text(d => d.title);

  g.append("text")
    .attr("x", 14).attr("y", 42)
    .attr("font-size", 13)
    .attr("opacity", 0.85)
    .text(d => d.sub);

  // ---- cycling highlight ----
  const hi = svg.append("rect")
    .attr("rx", 14)
    .attr("fill", "rgba(234,170,0,0.10)")
    .attr("stroke", "rgba(234,170,0,0.75)")
    .attr("stroke-width", 3)
    .attr("pointer-events","none");

  const cycle = ["mb","q0","flow","F","gth","gph"]; // matches pseudo-code order + both updates
  let idx = 0;

  function step(){
    const d = byId.get(cycle[idx]);
    hi.attr("x", d.x).attr("y", d.y).attr("width", d.w).attr("height", d.h);
    idx = (idx + 1) % cycle.length;
    setTimeout(step, 900);
  }
  step();

  // small label: while loop
  svg.append("text")
    .attr("x", 60)
    .attr("y", 28)
    .attr("font-size", 14)
    .attr("font-weight", 700)
    .attr("opacity", 0.8)
    .text("Repeat until converged");

  return svg.node();
}

``` 


### Computational complexity

A key advantage of normalizing flows is that
they add **minimal computational overhead**.

- Jacobian determinant computed in **$O(D)$**
- No matrix inverses required
- Total cost scales **linearly in flow length** $K$
- Compatible with minibatch SGD and backpropagation

---

##  Bayes Flows{#BFlowStart}

### Why BayesFlow?

Many scientific models are **simulator-based**:

- We can simulate data $x \sim p(x\mid \theta)$
- But the likelihood $p(x\mid \theta)$ is **intractable / unavailable**
- Goal: learn the **posterior** $p(\theta \mid x)$anyway

### Why this is hard

- Classical Bayesian inference relies on evaluating $p(x\mid \theta)$
- ABC / SMC-ABC can work, but is often:
  - slow (many simulations)
  - sensitive to handcrafted summaries / distances
  - hard to scale to high dimensions

**Key need:** fast, accurate **likelihood-free** Bayesian inference that can be reused across many datasets.

---

## BayesFlow in one idea: amortize inference

<div class="mizzou-code-box">
BayesFlow is a framework for globally amortized Bayesian inference that uses invertible neural networks (normalizing flows) and learned summary statistics to perform likelihood-free inference efficiently and accurately across many datasets.
</div>

<br><br>

### What BayesFlow learns (high level)

- A **summary network** learns informative representations of the dataset $\tilde{x} = h_\psi(x_{1:N})$
- A **conditional normalizing flow** learns an invertible transform so we can:
  - **sample** quickly from $\theta \sim p(\theta \mid x)$
  - **evaluate** posterior densities via Jacobians (for calibration and diagnostics)
  
---  

## BayesFlow: summary + inference networks

::: columns
::: column
### Core idea

BayesFlow learns the inverse mapping
$$
x_{1:N} \;\mapsto\; p(\theta \mid x_{1:N})
$$
**once** using simulations, and reuses it for fast inference.

<br>

### Summary network
$$
\tilde{x} = h_\psi(x_{1:N})
$$
- Converts variable-length data to fixed summaries  
- Learns informative statistics automatically  
- Architecture matches data structure  
  
:::

::: column  
### Inference network (flow-based)

$$
\theta \;\xleftrightarrow[]{\text{invertible}}\; z,
\qquad z \sim \mathcal{N}(0,I)
$$

- Conditional invertible neural network (cINN)  
- Learns a bijective transport map
$$
\theta = f_\phi(z \mid \tilde{x})
$$
- Enables:
  - exact density evaluation (Jacobian)
  - exact posterior sampling (inverse pass)
:::
:::
  
--- 

## Notation and setup

::: columns
::: column
We consider a generative model with:

- Parameter vector  
  $$
  \theta = (\theta_1,\ldots,\theta_D) \in \mathbb{R}^D
  $$

- Dataset of size $N$  $x_{1:N} = (x_1,\ldots,x_N)$
  - Fixed size $N$
  - Varying Size   $N$

<br>

### Neural components

- **Inference network (flow)** with parameters $\phi$
- **Summary network** with parameters $\psi$

::: 
::: column

<br><br><br>

### Latent space

We introduce a latent Gaussian variable
$$
z \sim \mathcal{N}(0, I),
$$
which will be mapped **invertibly** to $\theta$.

::: 
:::

## Learning the posterior via invertible maps

We seek an approximation
$$
p_\phi(\theta \mid x) \approx p(\theta \mid x)
$$
using an invertible neural network
$$
\theta = f_\phi(z| x),
\qquad
z \sim \mathcal{N}(0, I).
$$

<br>

### Change of variables

The induced posterior density is
$$
p_\phi(\theta \mid x)
=
p(z)\,
\left|
\det \frac{\partial f_\phi(z|x)}{\partial \theta}
\right|^{-1}.
$$



Thus, inference reduces to learning a **normalizing flow**
between $z$ and $\theta$, conditioned on data.

## Optimization objective

We minimize the expected KL divergence:
$$
\phi^\star
=
\arg\min_\phi
\mathbb{E}_{p(x)}
\big[
\mathrm{KL}(p(\theta\mid x)\,\|\,p_\phi(\theta\mid x))
\big].
$$



This is equivalent to maximizing:
$$
\mathbb{E}_{p(x,\theta)}
\big[
\log p_\phi(\theta \mid x)
\big].
$$



### Monte Carlo training

Using simulated pairs $(x^{(m)},\theta^{(m)})$:
$$
\mathcal{L}(\phi)
=
\frac{1}{M}\sum_{m=1}^M
\Big(
-\log p(z^{(m)})
- \log \big|\det J_{f_\phi}\big|
\Big).
$$

This objective is **fully tractable**.


## Variable-size datasets and summaries

When $N$ varies, raw data cannot be fed directly.

<br>

Introduce a learned summary:
$$
\tilde{x} = h_\psi(x_{1:N})
$$


The conditional flow becomes:
$$
\theta = f_\phi(z; \tilde{x}),
\qquad
\tilde{x} = h_\psi(x_{1:N}).
$$



### Joint objective

We now optimize:
$$
(\phi^\star,\psi^\star)
=
\arg\max_{\phi,\psi}
\mathbb{E}_{p(x,\theta)}
\big[
\log p_\phi(\theta \mid h_\psi(x_{1:N}))
\big].
$$

## Composing invertible networks

BayesFlow uses **chains of invertible transformations**
to increase expressiveness.


Let
$$
f_\phi = f_K \circ \cdots \circ f_1,
$$
so that
$$
\theta = f_K \circ \cdots \circ f_1(z).
$$



Each block $f_k$ is an **affine coupling block (ACB)**.



### Log-density

The log determinant decomposes:
$$
\log \left|\det J_{f_\phi}\right|
=
\sum_{k=1}^K
\log \left|\det J_{f_k}\right|.
$$

---

## Amortized Bayesian Inference With the BayesFlow Method {#AlgoBayesFlow}

```{ojs}

bayesflow_algo_anim575 = {
  const W = 1400, H = 520;

  const svg = d3.create("svg")
    .attr("viewBox", `0 0 ${W} ${H}`)
    .style("width","100%")
    .style("max-width", `${W}px`)
    .style("height", `${H}px`);

  // ---------- defs ----------
  const defs = svg.append("defs");

  defs.append("marker")
    .attr("id","arr")
    .attr("viewBox","0 0 10 10")
    .attr("refX", 9).attr("refY", 5)
    .attr("markerWidth", 8).attr("markerHeight", 8)
    .attr("orient","auto")
    .append("path")
    .attr("d","M 0 0 L 10 5 L 0 10 z")
    .attr("fill","rgba(0,0,0,0.55)");

  defs.append("filter").attr("id","sh")
    .append("feDropShadow")
      .attr("dx", 0).attr("dy", 1.2)
      .attr("stdDeviation", 1.2)
      .attr("flood-opacity", 0.18);

  const g = svg.append("g");

  // ---------- helpers ----------
  const box = (d, accent=false) => {
    const gg = g.append("g").attr("data-id", d.id);

    gg.append("rect")
      .attr("x", d.x).attr("y", d.y)
      .attr("width", d.w).attr("height", d.h)
      .attr("rx", 16)
      .attr("fill", "rgba(255,255,255,0.94)")
      .attr("stroke", accent ? "rgba(244,180,0,0.95)" : "rgba(0,0,0,0.16)")
      .attr("stroke-width", accent ? 3 : 1.6)
      .attr("filter","url(#sh)");

    gg.append("text")
      .attr("x", d.x + 16).attr("y", d.y + 28)
      .attr("font-size", 16).attr("font-weight", 850)
      .text(d.title);

    gg.append("text")
      .attr("x", d.x + 16).attr("y", d.y + 52)
      .attr("font-size", 12.5).attr("opacity", 0.82)
      .text(d.line1 || "");

    if (d.line2){
      gg.append("text")
        .attr("x", d.x + 16).attr("y", d.y + 72)
        .attr("font-size", 12.5).attr("opacity", 0.82)
        .text(d.line2);
    }
    return gg;
  };

  const center = d => [d.x + d.w/2, d.y + d.h/2];
  const midR   = d => [d.x + d.w,   d.y + d.h/2];
  const midL   = d => [d.x,         d.y + d.h/2];
  const midB   = d => [d.x + d.w/2, d.y + d.h];
  const midT   = d => [d.x + d.w/2, d.y];

  const curvedLR = (A,B) => {
    const [x1,y1] = A, [x2,y2] = B;
    const mx = (x1+x2)/2;
    return `M${x1},${y1} C${mx},${y1} ${mx},${y2} ${x2},${y2}`;
  };

  const curvedDown = (A,B, bow=60) => {
    const [x1,y1] = A, [x2,y2] = B;
    const my = (y1+y2)/2 + bow;
    return `M${x1},${y1} C${x1},${my} ${x2},${my} ${x2},${y2}`;
  };

  const loopBelow = (fromA, toB, yLaneBottom) => {
    const [x1,y1] = fromA;
    const [x2,y2] = toB;

    const yDown = yLaneBottom + 34;
    const pull = 140;

    return [
      `M${x1},${y1}`,
      `C${x1+pull},${y1} ${x1+pull},${yDown} ${x1},${yDown}`,
      `C${(x1+x2)/2},${yDown} ${(x1+x2)/2},${yDown} ${x2},${yDown}`,
      `C${x2-pull},${yDown} ${x2-pull},${y2} ${x2},${y2}`
    ].join(" ");
  };

  const loopBracket = ({x, y, w, h, label}) => {
    const gg = g.append("g");

    gg.append("rect")
      .attr("x", x).attr("y", y)
      .attr("width", w).attr("height", h)
      .attr("rx", 18)
      .attr("fill", "rgba(244,180,0,0.06)")
      .attr("stroke", "rgba(244,180,0,0.55)")
      .attr("stroke-width", 2)
      .attr("stroke-dasharray", "8,6");

    gg.append("text")
      .attr("x", x + 16).attr("y", y - 10)
      .attr("font-size", 13.5)
      .attr("font-weight", 850)
      .attr("opacity", 0.90)
      .text(label);

    return gg;
  };

  // ---------- layout ----------
  const yT = 120;
  const yI = 360;

  const train = [
    {id:"N",   x:70,   y:yT, w:210, h:92, title:"1) Sample N",   line1:"N ~ U(Nmin, Nmax)"},
    {id:"th",  x:320,  y:yT, w:210, h:92, title:"2) Sample Œ∏",   line1:"Œ∏^(m) ~ p(Œ∏)"},
    {id:"sim", x:570,  y:yT, w:230, h:92, title:"3) Simulate x", line1:"Œæ_i ~ p(Œæ), x_i = g(Œ∏^(m), Œæ_i)", line2:"for i = 1..N"},
    {id:"sum", x:840,  y:yT, w:210, h:92, title:"4) Summary",    line1:"xÃÉ = hœà(x1:N)"},
    {id:"fwd", x:1090, y:yT, w:210, h:92, title:"5) Flow (‚Üí)",   line1:"z = fœï(Œ∏; xÃÉ)"},
  ];

  const infer = [
    {id:"obs", x:70,   y:yI, w:250, h:92, title:"Inference input", line1:"observed x1:N"},
    {id:"sumo",x:360,  y:yI, w:210, h:92, title:"Summary",         line1:"xÃÉ = hœà(x1:N)"},
    {id:"z",   x:610,  y:yI, w:210, h:92, title:"Sample latent",   line1:"z(l) ~ N(0, I)"},
    {id:"inv", x:860,  y:yI, w:250, h:92, title:"Flow (‚Üê)",        line1:"Œ∏^(l) = fœï(z(l); xÃÉ)"},
    // tweak return to be explicit (optional)
    {id:"out", x:1150, y:yI, w:160, h:92, title:"Return",          line1:"Œ∏^(l) for L = 1..L"},
  ];

  const upd = {
    id:"updPanel", x:1040, y:235, w:260, h:96,
    title:"Training objective",
    line1:"compute loss ",
    line2:"update parameters: œï, œà"
  };

  // titles
  const tag = (x,y,text) => {
    g.append("text")
      .attr("x", x).attr("y", y)
      .attr("font-size", 14).attr("font-weight", 900)
      .attr("opacity", 0.85)
      .text(text);
  };
  tag(70, 95,  "TRAINING PHASE (repeat)");
  tag(70, 335, "INFERENCE PHASE (given observed data)");

  // entry/exit gates
  const gate = (x,y,label) => {
    const gg = g.append("g");
    gg.append("circle")
      .attr("cx",x).attr("cy",y).attr("r",10)
      .attr("fill","rgba(244,180,0,0.95)")
      .attr("stroke","rgba(0,0,0,0.18)");
    gg.append("text")
      .attr("x",x+16).attr("y",y+5)
      .attr("font-size", 13).attr("font-weight", 850)
      .attr("opacity", 0.85)
      .text(label);
  };
  gate(40, yT + 46, "enter repeat");
  gate(40, yI + 46, "exit with posterior samples");

  // draw boxes
  const nodes = [...train, ...infer, upd];
  const map = new Map(nodes.map(d => [d.id, d]));

  const nodeG = new Map();
  train.forEach(d => nodeG.set(d.id, box(d, false)));
  infer.forEach(d => nodeG.set(d.id, box(d, false)));
  nodeG.set(upd.id, box(upd, true));

  // loop annotations (m and i)
  const mLeft  = map.get("th").x - 18;
  const mTop   = map.get("th").y - 22;
  const mRight = map.get("fwd").x + map.get("fwd").w + 18;
  const mBot   = map.get("fwd").y + map.get("fwd").h + 18;

  loopBracket({
    x: mLeft,
    y: mTop,
    w: mRight - mLeft,
    h: mBot - mTop,
    label: "for m = 1..M  (mini-batch)"
  });

  const simBox = map.get("sim");
  loopBracket({
    x: simBox.x - 12,
    y: simBox.y - 14,
    w: simBox.w + 24,
    h: simBox.h + 28,
    label: "for i = 1..N  (simulate observations)"
  });

  // NEW: inference loop over l = 1..L (covers Sample latent + Flow inverse)
  const zBox = map.get("z");
  const invBox = map.get("inv");
  const lLeft  = zBox.x - 14;
  const lTop   = zBox.y - 22;
  const lRight = invBox.x + invBox.w + 14;
  const lBot   = invBox.y + invBox.h + 18;

  loopBracket({
    x: lLeft,
    y: lTop,
    w: lRight - lLeft,
    h: lBot - lTop,
    label: "for l = 1..L  (posterior samples)"
  });

  // main lane arrows
  const drawEdges = (arr) => {
    g.append("g")
      .selectAll("path.e")
      .data(arr)
      .join("path")
        .attr("fill","none")
        .attr("stroke","rgba(0,0,0,0.28)")
        .attr("stroke-width", 2.2)
        .attr("marker-end","url(#arr)")
        .attr("d", ([a,b]) => curvedLR(midR(map.get(a)), midL(map.get(b))));
  };

  drawEdges([["N","th"],["th","sim"],["sim","sum"],["sum","fwd"]]);
  drawEdges([["obs","sumo"],["sumo","z"],["z","inv"],["inv","out"]]);

  // Flow -> Training objective
  g.append("path")
    .attr("fill","none")
    .attr("stroke","rgba(0,0,0,0.28)")
    .attr("stroke-width", 2.2)
    .attr("marker-end","url(#arr)")
    .attr("d", curvedDown(midB(map.get("fwd")), midT(map.get("updPanel")), 70));

  // repeat loop: objective -> Sample N (under boxes)
  const yTrainBottom = yT + 92;
  g.append("path")
    .attr("fill","none")
    .attr("stroke","rgba(0,0,0,0.18)")
    .attr("stroke-width", 2.1)
    .attr("marker-end","url(#arr)")
    .attr("d", loopBelow(midL(map.get("updPanel")), midL(map.get("N")), yTrainBottom));

  // ---------- animation ----------
  const steps = [
    "N","th","sim","sum","fwd","updPanel","N",
    "obs","sumo","z","inv","out"
  ];

  const packet = g.append("circle")
    .attr("r", 6)
    .attr("fill","rgba(0,0,0,0.65)")
    .attr("opacity", 0.85);

  function setAccent(id){
    [...train, ...infer].forEach(d => {
      nodeG.get(d.id).select("rect")
        .attr("stroke","rgba(0,0,0,0.16)")
        .attr("stroke-width", 1.6);
    });
    nodeG.get("updPanel").select("rect")
      .attr("stroke","rgba(244,180,0,0.95)")
      .attr("stroke-width", 3);

    if (id !== "updPanel"){
      nodeG.get(id).select("rect")
        .attr("stroke","rgba(244,180,0,0.95)")
        .attr("stroke-width", 3);
    }
  }

  const lerp = (a,b,t) => a + (b-a)*t;

  function tick(tms){
    const T = 1200;
    const k = Math.floor((tms / T) % steps.length);
    const t = (tms / T) % 1;

    const a = steps[k];
    const b = steps[(k+1) % steps.length];

    setAccent(a);

    const A = center(map.get(a));
    const B = center(map.get(b));

    packet
      .attr("cx", lerp(A[0], B[0], t))
      .attr("cy", lerp(A[1], B[1], t));

    requestAnimationFrame(tick);
  }
  requestAnimationFrame(tick);

  return svg.node();
}


```

---




## Notation and setup

::: columns
::: column
### Normalizing Flows
Learn an invertible transformation between a simple base distribution and a complex target distribution.

$$
z = f_\theta(x), \qquad x = f_\theta^{-1}(z)
$$

Uses the change-of-variables formula:
$$
p(x) = p(z)\left|\det \frac{\partial f_\theta(x)}{\partial x}\right|
$$

Model complex probability densities by warping a simple distribution.

It learns a single invertible mapping $f_\theta$ with fixed parameters.


Inference is not amortized by default.
A new dataset typically requires retraining or re-optimization.

::: 
::: column

### Bayesian Flows (BayesFlow)
Learn an amortized inverse mapping from data to posterior distributions using simulations.

$$
x_{1:N} \;\longmapsto\; p(\theta \mid x_{1:N})
$$

1. Summary network $\tilde{x} = h_\psi(x_{1:N})$

2. Conditional invertible inference network
$$
\theta = f_\phi^{-1}(z; \tilde{x}), \qquad z \sim \mathcal{N}(0, I)
$$

Minimize the KL divergence between the true and approximate posterior using simulated data.

It leanrs A reusable inference network and a summary network aligned with probabilistic symmetries

Inference is amortized.
After training, posterior sampling is near-instant for new datasets.

::: 
:::


---

## Recent Developments in Normalizing Flows {#DevelopNormFlow}

Modern normalizing flows extend the basic idea of invertible density
transformations to achieve **scalability, expressiveness, and stability**
in high dimensions.


### Discrete-Time Flow Architectures

Most practical flows use **layered, discrete transformations**:
$$
z_K = f_K \circ f_{K-1} \circ \cdots \circ f_1(z_0),
\qquad z_0 \sim p_0(z)
$$

### Coupling-based flows

Split variables into two blocks:
$$
(x_A, x_B) \;\mapsto\; (y_A, y_B)
$$
with
$$
y_A = h(x_A; \Theta(x_B)), \qquad y_B = x_B.
$$

**Key property:**
- Triangular Jacobian
- $\log|\det J|$ computed cheaply

**Examples:** NICE, RealNVP, Glow



## Autoregressive and Spline Flows

### Autoregressive flows

Each dimension depends on previous ones:
$$
y_t = h\big(x_t;\,\Theta_t(x_{1:t-1})\big)
$$

- **MAF**: fast density evaluation  
- **IAF**: fast sampling  

<br> <br>

### Neural spline flows

Replace affine maps with **monotone splines**:

- Rational quadratic splines
- Piecewise polynomial transforms

**Key insight:**
> Nonlinear monotone transformations dramatically increase expressiveness
without sacrificing invertibility.

---

## Continuous-Time Normalizing Flows

Flows can be defined as **continuous dynamics**:
$$
\frac{dx(t)}{dt} = F(x(t), t)
$$

Density evolves via:
$$
\frac{d}{dt} \log p(x(t))
=
-\mathrm{Tr}\!\left(\frac{\partial F}{\partial x}\right)
$$

**Representative methods:**

- Neural ODE flows
- FFJORD (stochastic trace estimator)

**Advantages:**

- No explicit Jacobian determinant
- Parameter-efficient
- Smooth transformations

---

## Summary of Normalizing Flow Families (I) {#SummNormFlow}

| Flow family | Core transformation | Key idea | Strengths | Limitations |
|------------|--------------------|----------|-----------|-------------|
| Elementwise bijections | $$z_i = f_i(x_i)$$ | Independent nonlinear transforms | Simple, cheap Jacobian | No variable mixing |
| Linear flows | $$z = Ax + b$$ | Global affine transform | Mixes variables, exact inverse | Limited expressiveness |
| Planar flows | $$z = x + u\,h(w^\top x + b)$$ | Rank-1 nonlinear deformation | Flexible, fast Jacobian | Inverse often intractable |
| Radial flows | $$z = x + \beta\,h(\alpha, r)(x - x_0)$$ | Radial contraction/expansion | Can induce multimodality | Hard inverse |
| Coupling flows | $$z_{1:d}=x_{1:d}$$ $$z_{d+1:D}=x_{d+1:D}\odot e^{s(x_{1:d})}+t(x_{1:d})$$ | Split variables, conditional transforms | Exact inverse, scalable | Needs good coupling design |


## Summary of Normalizing Flow Families (II)
<br><br>

| Flow family | Core transformation | Key idea | Strengths | Limitations |
|------------|--------------------|----------|-----------|-------------|
| Autoregressive flows | $$z_i = f_i(x_i; x_{<i})$$ | Sequential conditional transforms | Highly expressive | Slow sampling or likelihood |
| Residual flows | $$z = x + f(x)$$ | Invertible residual networks | Deep & flexible | Requires Lipschitz control |
| Continuous (Neural ODE) flows | $$\frac{dz(t)}{dt} = f(z(t), t)$$ | Continuous-time transformation | Memory efficient, smooth | Expensive ODE solves |

---


## Overview of Coupling flows

```{ojs}
nf_methods_timeline = {
  const W = 1000, H = 440;
  const svg = d3.create("svg")
    .attr("viewBox", `0 0 ${W} ${H}`)
    .style("width","100%")
    .style("max-width","980px")
    .style("height","440px");

  // ---------- defs ----------
  const defs = svg.append("defs");

  defs.append("marker")
    .attr("id","arrow")
    .attr("viewBox","0 0 10 10")
    .attr("refX", 9).attr("refY", 5)
    .attr("markerWidth", 8).attr("markerHeight", 8)
    .attr("orient","auto")
    .append("path")
    .attr("d","M 0 0 L 10 5 L 0 10 z")
    .attr("fill","rgba(0,0,0,0.55)");

  defs.append("filter").attr("id","glow")
    .append("feDropShadow")
      .attr("dx", 0).attr("dy", 0)
      .attr("stdDeviation", 2.2)
      .attr("flood-opacity", 0.30);

  // ---------- frame ----------
  svg.append("rect")
    .attr("x", 18).attr("y", 26)
    .attr("width", W-36).attr("height", H-52)
    .attr("rx", 10)
    .attr("fill","white")
    .attr("stroke","rgba(0,0,0,0.25)");

  

  // ---------- left-side taxonomy ----------
  const items = [
    {sec:"¬ß3.1", title:"Elementwise bijections", sub:"Non-linear elementwise transform",
     prob:"Problem: no mixing of variables", color:"#b00020"},
    {sec:"¬ß3.2", title:"Linear flows", sub:"Affine combination of variables",
     prob:"Problem: limited representational power", color:"#b00020"},
    {sec:"¬ß3.3", title:"Planar and radial flows", sub:"Non-linear transforms",
     prob:"Problem: hard to compute inverse", color:"#b00020"},
    {sec:"¬ß3.4.1", title:"Coupling flows",
     sub:"Architectures that allow invertible non-linear transformations",
     prob:"Depend on coupling", color:"#1b7f3a"},
    {sec:"¬ß3.4.2", title:"Autoregressive flows",
     sub:"Architectures that allow invertible non-linear transformations",
     prob:"Depend on coupling", color:"#1b7f3a"},
    {sec:"¬ß3.5", title:"Residual flows", sub:"Invertible residual networks",
     prob:"", color:"#1b7f3a"},
    {sec:"¬ß3.6", title:"Infinitesimal flows",
     sub:"Continuous flows depending on ODEs or SDEs",
     prob:"", color:"#1b7f3a"}
  ];

  const leftX = 42;
  const startY = 70;
  const rowH = 50;

  const rows = svg.append("g")
    .selectAll("g.row")
    .data(items)
    .join("g")
    .attr("transform", (d,i)=>`translate(${leftX},${startY+i*rowH})`);

  rows.append("rect")
    .attr("x",-12).attr("y",-18)
    .attr("width",460).attr("height",42)
    .attr("rx",10)
    .attr("fill","rgba(244,180,0,0.08)")
    .attr("stroke","rgba(244,180,0,0.85)")
    .attr("stroke-width",2.5)
    .attr("opacity",0)
    .attr("filter","url(#glow)");

  rows.append("text")
    .attr("x",0).attr("y",-2)
    .attr("font-size",16)
    .attr("font-weight",700)
    .text(d=>`${d.sec}  ${d.title}`);

  rows.append("text")
    .attr("x",20).attr("y",16)
    .attr("font-size",12.5)
    .attr("fill","#1b7f3a")
    .text(d=>d.sub);

  rows.append("line")
    .attr("x1",330).attr("x2",380)
    .attr("y1",-6).attr("y2",-6)
    .attr("stroke","rgba(0,0,0,0.5)")
    .attr("stroke-width",1.8);

  rows.append("path")
    .attr("d","M380,-6 L450,-6")
    .attr("stroke","rgba(0,0,0,0.5)")
    .attr("stroke-width",1.8)
    .attr("marker-end","url(#arrow)");

  rows.append("text")
    .attr("x",462).attr("y",-2)
    .attr("font-size",12.5)
    .attr("font-weight",650)
    .attr("fill",d=>d.color)
    .text(d=>d.prob);

  // ---------- coupling-functions box (MOVED DOWN) ----------
  const cf = {
    x: 640,
    y: 210,          // << MOVED DOWN
    w: 300,
    h: 210,
    bullets: [
      "Affine",
      "Non-linear squared",
      "Continuous mixture CDFs",
      "Splines",
      "Neural autoregressive",
      "Sum-of-squares polynomial",
      "Piecewise-bijective"
    ]
  };

  svg.append("rect")
    .attr("x",cf.x).attr("y",cf.y)
    .attr("width",cf.w).attr("height",cf.h)
    .attr("rx",10)
    .attr("fill","white")
    .attr("stroke","rgba(0,0,0,0.45)")
    .attr("stroke-width",1.6);

  svg.append("text")
    .attr("x",cf.x+14).attr("y",cf.y+26)
    .attr("font-size",15)
    .attr("font-weight",750)
    .text("¬ß3.4.4  Coupling functions");

  svg.selectAll("text.cf")
    .data(cf.bullets)
    .join("text")
      .attr("x",cf.x+18)
      .attr("y",(d,i)=>cf.y+54+i*22)
      .attr("font-size",13)
      .text(d=>"‚Ä¢ "+d);

  // ---------- connectors ----------
  function connector(y0){
    const x1 = leftX + 380;
    const x2 = cf.x - 12;
    const y2 = cf.y + 22;
    const mx = (x1+x2)/2 + 40;
    return `M${x1},${y0} C${mx},${y0} ${mx},${y2} ${x2},${y2}`;
  }

  const conn = svg.append("path")
    .attr("fill","none")
    .attr("stroke","rgba(0,0,0,0.35)")
    .attr("stroke-width",2)
    .attr("marker-end","url(#arrow)");

  // ---------- animation ----------
  let k = 0;
  const T = 1700;

  function focus(i){
    rows.select("rect").attr("opacity",0);
    rows.filter((d,idx)=>idx===i)
      .select("rect").attr("opacity",1);

    if (i===3 || i===4){
      const y0 = startY + i*rowH - 6;
      conn.attr("d",connector(y0)).attr("opacity",1);
    } else {
      conn.attr("opacity",0);
    }
  }

  function loop(){
    focus(k);
    k = (k+1)%items.length;
    d3.timeout(loop,T);
  }

  focus(0);
  d3.timeout(loop,T);

  return svg.node();
}
```

---

## Future Directions of Normalizing Flows {#FutureDir}

**Next-Generation Generative Modeling & Inference**

-   **Continuous-Time Flows:** Moving beyond discrete layers to Neural ODEs for more flexible, invertible transformations.
-  **Geometric & Graph Flows:** Adapting flows to non-Euclidean data (graphs, manifolds) for structural biology and chemistry.
-   **Efficiency & Scalability:** Reducing computational overhead for high-dimensional data (e.g., high-res images, 3D modeling).
-   **Hybrid Architectures:** Combining flows with diffusion models or VAEs to leverage exact likelihoods and high-fidelity generation.
-   **Beyond Gaussian Base:** Exploring more expressive, non-Gaussian, or learnable base distributions.

<br>

### Key Research Areas
1.  Riemannian Manifold Flows
2.  Adaptive Step Size ODEs
3.  Implicit Regularization Techniques

---

## References

<br>

These are links to a few papers on Normalizing flows I found interesting:
<br><br> <br>

[Rezende, D., & Mohamed, S. (2015, June). Variational inference with normalizing flows. In International conference on machine learning (pp. 1530-1538). PMLR.](https://nam02.safelinks.protection.outlook.com/?url=https%3A%2F%2Fproceedings.mlr.press%2Fv37%2Frezende15&data=05%7C02%7Cabt7v%40missouri.edu%7C356f0a680e0f4875982a08de5e054c17%7Ce3fefdbef7e9401ba51a355e01b05a89%7C0%7C0%7C639051573990741510%7CUnknown%7CTWFpbGZsb3d8eyJFbXB0eU1hcGkiOnRydWUsIlYiOiIwLjAuMDAwMCIsIlAiOiJXaW4zMiIsIkFOIjoiTWFpbCIsIldUIjoyfQ%3D%3D%7C0%7C%7C%7C&sdata=kXt%2B2DzXfF7rURnaMaKL3PaBoXrRH4SBv4ydIPcwDMg%3D&reserved=0)


[Radev, S. T., Mertens, U. K., Voss, A., Ardizzone, L., & K√∂the, U. (2020). BayesFlow: Learning complex stochastic models with invertible neural networks. IEEE transactions on neural networks and learning systems, 33(4), 1452-1466.](https://nam02.safelinks.protection.outlook.com/?url=https%3A%2F%2Fieeexplore.ieee.org%2Fabstract%2Fdocument%2F9298920%3Fcasa_token%3DNXzFiO8sneoAAAAA%3A-TawPGyF2UhmdJ72nCavtBJ8NVcuJtZ7dALtm_WteHPp69od53k5Kq5MZyoOiZ_TgWroKmM&data=05%7C02%7Cabt7v%40missouri.edu%7C356f0a680e0f4875982a08de5e054c17%7Ce3fefdbef7e9401ba51a355e01b05a89%7C0%7C0%7C639051573990769789%7CUnknown%7CTWFpbGZsb3d8eyJFbXB0eU1hcGkiOnRydWUsIlYiOiIwLjAuMDAwMCIsIlAiOiJXaW4zMiIsIkFOIjoiTWFpbCIsIldUIjoyfQ%3D%3D%7C0%7C%7C%7C&sdata=ZLgDyiCwbF6aqtx2R5UcM6yFqCPOruC6ItmRgHh%2FHwQ%3D&reserved=0)


[Ivan Kobyzev, Simon J.D. Prince, Marcus A. Brubaker (2019). Normalizing Flows: An Introduction and Review of Current Methods](https://arxiv.org/abs/1908.09257)






---


##  {data-background-image="assets/tigerstripes.png" data-background-size="cover" data-background-position="center"}

<div class="thankyou-slide">

<div class="thankyou-content">

Thank you for your attention!


</div>

</div>


